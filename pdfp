import os
import json
import tempfile
import streamlit as st
import qdrant_client
from pathlib import Path
from datetime import datetime
from llama_index.core.schema import Document
from llama_index.core.indices.vector_store import VectorStoreIndex
from llama_index.core.storage.storage_context import StorageContext
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.mistralai import MistralAI
from llama_index.core.postprocessor import SentenceTransformerRerank
from llama_index.core.prompts import PromptTemplate
from llama_index.core.settings import Settings
import cv2
import numpy as np
import requests
import easyocr
import traceback
import logging
from PIL import Image
import io
import time
import fitz  # PyMuPDF
from pdf2image import convert_from_path, convert_from_bytes
import tempfile
import shutil
import hashlib

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ================================
# PAGE CONFIG & STYLING
# ================================

st.set_page_config(
    page_title="üè• Medical Report Analytics",
    page_icon="üè•",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better UI
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #4CAF50 0%, #2196F3 100%);
        padding: 2rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background: white;
        padding: 1rem;
        border-radius: 10px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        border-left: 4px solid #4CAF50;
    }
    .chat-container {
        background: #f8f9fa;
        padding: 1rem;
        border-radius: 10px;
        margin: 1rem 0;
    }
    .stButton > button {
        background: linear-gradient(90deg, #4CAF50 0%, #2196F3 100%);
        color: white;
        border: none;
        border-radius: 20px;
        padding: 0.5rem 2rem;
        font-weight: 600;
    }
    .upload-section {
        border: 2px dashed #4CAF50;
        border-radius: 10px;
        padding: 2rem;
        text-align: center;
        margin: 1rem 0;
    }
    .error-details {
        background: #ffe6e6;
        border: 1px solid #ff9999;
        border-radius: 5px;
        padding: 10px;
        margin: 10px 0;
    }
</style>
""", unsafe_allow_html=True)

# ================================
# ENHANCED MEDICAL OCR CLASS
# ================================

class MedicalReportOCR:
    def __init__(self, ollama_url="http://localhost:11434", model_name="llama3.2:1b"):
        self.ollama_url = ollama_url
        self.model_name = model_name
        self.ocr_reader = None
        
        # Initialize EasyOCR with better error handling
        self._init_ocr()
        
        # Test Ollama connection
        self._test_ollama_connection()
    
    def _init_ocr(self):
        """Initialize EasyOCR with proper error handling"""
        try:
            with st.spinner("Initializing OCR engine..."):
                self.ocr_reader = easyocr.Reader(['en'], gpu=False)  # Disable GPU to avoid issues
            logger.info("EasyOCR initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize EasyOCR: {e}")
            st.error(f"Failed to initialize EasyOCR: {e}")
            raise
    
    def _test_ollama_connection(self):
        """Test Ollama connection and model availability with timeout"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=10)
            if response.status_code == 200:
                models_data = response.json()
                available_models = [model['name'] for model in models_data.get('models', [])]
                
                if self.model_name in available_models:
                    st.success(f"‚úÖ Ollama connected - Model {self.model_name} ready")
                else:
                    st.warning(f"‚ö†Ô∏è Model {self.model_name} not found. Available: {available_models}")
                    st.info(f"Run: `ollama pull {self.model_name}` to download the model")
            else:
                st.error(f"‚ùå Ollama API returned status {response.status_code}")
        except requests.exceptions.Timeout:
            st.error("‚ùå Ollama connection timeout - check if Ollama is running")
        except requests.exceptions.ConnectionError:
            st.error("‚ùå Cannot connect to Ollama - ensure it's running on the correct port")
        except Exception as e:
            st.error(f"‚ùå Ollama connection error: {e}")
            st.info("Make sure Ollama is running: `ollama serve`")
    
    def _validate_image(self, image_path):
        """Validate image file before processing"""
        try:
            # Check if file exists
            if not os.path.exists(image_path):
                return False, "Image file not found"
            
            # Check file size (max 50MB)
            file_size = os.path.getsize(image_path)
            if file_size > 50 * 1024 * 1024:
                return False, "Image file too large (max 50MB)"
            
            # Try to open with PIL to validate
            with Image.open(image_path) as img:
                img.verify()
            
            # Try to read with OpenCV
            test_img = cv2.imread(image_path)
            if test_img is None:
                return False, "Cannot read image file with OpenCV"
            
            return True, "Valid image"
            
        except Exception as e:
            return False, f"Image validation error: {str(e)}"
    
    def _validate_pdf(self, pdf_path):
        """Validate PDF file before processing"""
        try:
            # Check if file exists
            if not os.path.exists(pdf_path):
                return False, "PDF file not found"
            
            # Check file size (max 100MB for PDF)
            file_size = os.path.getsize(pdf_path)
            if file_size > 100 * 1024 * 1024:
                return False, "PDF file too large (max 100MB)"
            
            # Try to open with PyMuPDF
            doc = fitz.open(pdf_path)
            page_count = len(doc)
            doc.close()
            
            if page_count == 0:
                return False, "PDF has no pages"
            
            if page_count > 50:
                return False, "PDF has too many pages (max 50)"
            
            return True, f"Valid PDF with {page_count} pages"
            
        except Exception as e:
            return False, f"PDF validation error: {str(e)}"

    def preprocess_image(self, image_path):
        """Enhanced image preprocessing with error handling"""
        try:
            # Validate image first
            is_valid, validation_msg = self._validate_image(image_path)
            if not is_valid:
                raise ValueError(validation_msg)
            
            # Read image
            img = cv2.imread(image_path)
            if img is None:
                raise ValueError(f"Could not read image: {image_path}")
            
            # Get image dimensions
            height, width = img.shape[:2]
            logger.info(f"Image dimensions: {width}x{height}")
            
            # Resize if too large (keep aspect ratio)
            max_dimension = 2000
            if max(height, width) > max_dimension:
                scale = max_dimension / max(height, width)
                new_width = int(width * scale)
                new_height = int(height * scale)
                img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_AREA)
                logger.info(f"Resized image to: {new_width}x{new_height}")
            
            # Convert to grayscale
            if len(img.shape) == 3:
                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            else:
                gray = img
            
            # Apply denoising
            denoised = cv2.fastNlMeansDenoising(gray)
            
            # Enhance contrast using CLAHE
            clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
            enhanced = clahe.apply(denoised)
            
            return enhanced
            
        except Exception as e:
            logger.error(f"Image preprocessing error: {e}")
            raise
    
    def extract_text_easyocr(self, image_path):
        """Enhanced text extraction with better error handling"""
        try:
            if self.ocr_reader is None:
                raise ValueError("OCR reader not initialized")
            
            # Process image with enhanced preprocessing
            processed_img = self.preprocess_image(image_path)
            
            # Extract text using EasyOCR with timeout
            logger.info("Starting OCR text extraction...")
            results = self.ocr_reader.readtext(processed_img)
            
            extracted_texts = []
            full_text_parts = []
            
            # Filter and process results with better confidence threshold
            for (bbox, text, confidence) in results:
                if confidence > 0.2:  # Lower threshold for medical text
                    cleaned_text = text.strip()
                    if cleaned_text and len(cleaned_text) > 1:
                        # Remove obvious OCR errors
                        if not self._is_garbage_text(cleaned_text):
                            extracted_texts.append({
                                'text': cleaned_text,
                                'confidence': round(confidence * 100, 2),
                                'bbox': bbox
                            })
                            full_text_parts.append(cleaned_text)
            
            # Combine all text with proper spacing
            full_text = ' '.join(full_text_parts)
            
            # Clean up common OCR errors
            full_text = self._clean_ocr_text(full_text)
            
            logger.info(f"Extracted {len(extracted_texts)} text blocks, {len(full_text)} characters total")
            
            return full_text, extracted_texts
            
        except Exception as e:
            logger.error(f"OCR extraction failed: {e}")
            return "", []
    def extract_text_from_pdf(self, pdf_path):
        """Extract text from PDF using PyMuPDF and OCR for images"""
        try:
            # First, try to extract text directly from PDF
            doc = fitz.open(pdf_path)
            extracted_text = ""
            has_text = False
            
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                page_text = page.get_text()
                
                if page_text.strip():
                    extracted_text += f"\n--- Page {page_num + 1} ---\n"
                    extracted_text += page_text
                    has_text = True
            
            doc.close()
            
            # If PDF has extractable text, return it
            if has_text and len(extracted_text.strip()) > 50:
                logger.info(f"Extracted text directly from PDF: {len(extracted_text)} characters")
                return extracted_text.strip(), []
            
            # If no text or very little text, convert PDF to images and use OCR
            logger.info("PDF has no extractable text, converting to images for OCR...")
            return self._pdf_to_images_ocr(pdf_path)
            
        except Exception as e:
            logger.error(f"PDF text extraction error: {e}")
            # Fallback to image conversion
            return self._pdf_to_images_ocr(pdf_path)
    
    def _pdf_to_images_ocr(self, pdf_path):
        """Convert PDF pages to images and extract text using OCR"""
        try:
            # Convert PDF to images
            with tempfile.TemporaryDirectory() as temp_dir:
                # Convert PDF pages to images
                images = convert_from_path(
                    pdf_path, 
                    dpi=300,  # High DPI for better OCR
                    output_folder=temp_dir,
                    fmt='jpeg',
                    thread_count=2
                )
                
                all_text_parts = []
                all_extraction_details = []
                
                for i, image in enumerate(images):
                    # Save image temporarily
                    image_path = os.path.join(temp_dir, f"page_{i+1}.jpg")
                    image.save(image_path, 'JPEG', quality=95)
                    
                    # Extract text using OCR
                    page_text, page_details = self.extract_text_easyocr(image_path)
                    
                    if page_text.strip():
                        all_text_parts.append(f"\n--- Page {i + 1} ---\n{page_text}")
                        
                        # Add page info to extraction details
                        for detail in page_details:
                            detail['page_number'] = i + 1
                        all_extraction_details.extend(page_details)
                
                combined_text = "\n".join(all_text_parts)
                logger.info(f"OCR extracted {len(combined_text)} characters from {len(images)} pages")
                
                return combined_text, all_extraction_details
                
        except Exception as e:
            logger.error(f"PDF to images OCR error: {e}")
            return "", []
    
    def process_file(self, file_path):
        """Universal file processor that handles both images and PDFs"""
        file_ext = os.path.splitext(file_path)[1].lower()
        
        if file_ext == '.pdf':
            return self.process_pdf(file_path)
        elif file_ext in ['.png', '.jpg', '.jpeg']:
            return self.process_image(file_path)
        else:
            return {
                'success': False,
                'error': f'Unsupported file type: {file_ext}',
                'filename': os.path.basename(file_path)
            }
    
    def _is_garbage_text(self, text):
        """Filter out garbage OCR text"""
        # Filter very short text
        if len(text.strip()) < 2:
            return True
        
        # Filter text with too many special characters
        special_chars = sum(1 for c in text if not c.isalnum() and not c.isspace())
        if special_chars > len(text) * 0.5:
            return True
        
        # Filter repeated single characters
        if len(set(text.replace(' ', ''))) == 1:
            return True
        
        return False
    
    def _clean_ocr_text(self, text):
        """Clean common OCR errors in medical text"""
        import re
        
        if not text:
            return text
        
        # Common OCR corrections for medical text
        corrections = {
            r'\b0\b': 'O',  # Zero to O
            r'\bl\b': 'I',  # lowercase l to I
            r'(\d+)\s*-\s*(\d+)': r'\1-\2',  # Fix range formatting
            r'(\d+)\s*\.\s*(\d+)': r'\1.\2',  # Fix decimal numbers
            r'\s+': ' ',  # Multiple spaces to single space
        }
        
        cleaned_text = text
        for pattern, replacement in corrections.items():
            try:
                cleaned_text = re.sub(pattern, replacement, cleaned_text)
            except:
                continue
        
        return cleaned_text.strip()
    
    def generate_json_with_ollama(self, extracted_text, image_filename):
        """Enhanced JSON generation with better error handling"""
        
        if not extracted_text or len(extracted_text.strip()) < 10:
            return {
                'success': False,
                'error': 'Insufficient text extracted from image for analysis',
                'raw_response': None
            }
        
        # Truncate text if too long
        max_text_length = 4000
        if len(extracted_text) > max_text_length:
            extracted_text = extracted_text[:max_text_length] + "\n[TEXT TRUNCATED]"
        
        # Simplified prompt for better success rate
        prompt = f"""Extract medical report information from this OCR text and format as JSON:

TEXT: {extracted_text}

Create JSON with these fields (use null if not found):
{{
  "hospital_info": {{
    "hospital_name": "string or null",
    "address": "string or null"
  }},
  "patient_info": {{
    "name": "string or null",
    "age": "string or null",
    "gender": "string or null"
  }},
  "doctor_info": {{
    "referring_doctor": "string or null"
  }},
  "report_info": {{
    "report_type": "string or null",
    "report_date": "string or null"
  }},
  "test_results": [
    {{
      "test_name": "string",
      "result_value": "string",
      "reference_range": "string or null",
      "unit": "string or null"
    }}
  ]
}}

Return only valid JSON, no extra text."""

        try:
            request_data = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "top_p": 0.9,
                    "num_predict": 1024,
                    "stop": ["```", "END", "STOP"]  # Add stop tokens
                }
            }
            
            logger.info("Sending request to Ollama...")
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=request_data,
                timeout=240,  # Reduced timeout
                headers={'Content-Type': 'application/json'}
            )
            
            if response.status_code != 200:
                return {
                    'success': False,
                    'error': f'Ollama API error: HTTP {response.status_code} - {response.text[:200]}',
                    'raw_response': response.text
                }
            
            try:
                result = response.json()
            except json.JSONDecodeError as e:
                return {
                    'success': False,
                    'error': f'Invalid JSON response from Ollama: {str(e)}',
                    'raw_response': response.text[:500]
                }
            
            json_text = result.get('response', '').strip()
            
            if not json_text:
                return {
                    'success': False,
                    'error': 'Empty response from Ollama',
                    'raw_response': str(result)
                }
            
            # Clean and extract JSON with better error handling
            try:
                json_text = self._extract_and_clean_json(json_text)
                parsed_json = json.loads(json_text)
            except json.JSONDecodeError as e:
                # Try to fix and parse again
                try:
                    fixed_json = self._fix_json_errors(json_text)
                    parsed_json = json.loads(fixed_json)
                except json.JSONDecodeError:
                    # Return a basic structure if parsing fails completely
                    logger.warning(f"JSON parsing failed for {image_filename}, using fallback structure")
                    parsed_json = {
                        "hospital_info": {"hospital_name": None, "address": None},
                        "patient_info": {"name": None, "age": None, "gender": None},
                        "doctor_info": {"referring_doctor": None},
                        "report_info": {"report_type": "Medical Report", "report_date": None},
                        "test_results": [],
                        "_extraction_note": "Partial extraction due to JSON parsing issues",
                        "_raw_text": extracted_text[:500] if len(extracted_text) > 500 else extracted_text
                    }
            
            # Add metadata
            parsed_json['_metadata'] = {
                'source_image': image_filename,
                'extraction_method': 'easyocr_ollama_enhanced',
                'processing_timestamp': datetime.now().isoformat(),
                'model_used': self.model_name,
                'text_length': len(extracted_text)
            }
            
            return {
                'success': True,
                'json_data': parsed_json,
                'raw_response': json_text
            }
            
        except requests.exceptions.Timeout:
            return {
                'success': False,
                'error': 'Ollama request timeout - server may be overloaded',
                'raw_response': None
            }
        except requests.exceptions.ConnectionError:
            return {
                'success': False,
                'error': 'Cannot connect to Ollama - ensure it is running on http://localhost:11434',
                'raw_response': None
            }
        except Exception as e:
            return {
                'success': False,
                'error': f'Unexpected error in Ollama processing: {str(e)}',
                'raw_response': None
            }
            
    def _extract_and_clean_json(self, json_text):
        """Extract and clean JSON from response"""
        import re
        
        # Remove markdown code blocks
        if '```json' in json_text:
            json_text = json_text.split('```json')[1].split('```')[0]
        elif '```' in json_text:
            json_text = json_text.split('```')[1].split('```')[0]
        
        json_text = json_text.strip()
        
        # Find JSON object
        if not json_text.startswith('{'):
            json_match = re.search(r'(\{.*\})', json_text, re.DOTALL)
            if json_match:
                json_text = json_match.group(1)
        
        return json_text
    
    def _fix_json_errors(self, json_text):
        """Attempt to fix common JSON errors"""
        import re
        
        fixes = [
            (r',\s*}', '}'),  # Remove trailing commas before }
            (r',\s*]', ']'),  # Remove trailing commas before ]
            (r':\s*,', ': null,'),  # Fix empty values
            (r':\s*}', ': null}'),  # Fix empty values at end
        ]
        
        fixed_json = json_text
        for pattern, replacement in fixes:
            fixed_json = re.sub(pattern, replacement, fixed_json)
        
        return fixed_json
    
    def process_image(self, image_path):
        """Process image with comprehensive error handling"""
        image_filename = os.path.basename(image_path)
        
        try:
            logger.info(f"Processing image: {image_filename}")
            
            # Validate image first
            is_valid, validation_msg = self._validate_image(image_path)
            if not is_valid:
                return {
                    'success': False,
                    'error': f'Image validation failed: {validation_msg}',
                    'image_filename': image_filename
                }
            
            # Extract text
            extracted_text, extraction_details = self.extract_text_easyocr(image_path)
            
            if not extracted_text.strip():
                return {
                    'success': False,
                    'error': 'No readable text found in image',
                    'image_filename': image_filename,
                    'extraction_details': extraction_details
                }
            
            # Generate structured JSON
            ollama_result = self.generate_json_with_ollama(extracted_text, image_filename)
            
            if ollama_result['success']:
                return {
                    'success': True,
                    'image_filename': image_filename,
                    'extracted_text': extracted_text,
                    'extraction_details': extraction_details,
                    'structured_json': ollama_result['json_data'],
                    'text_blocks_count': len(extraction_details),
                    'text_confidence_avg': sum(item['confidence'] for item in extraction_details) / len(extraction_details) if extraction_details else 0
                }
            else:
                return {
                    'success': False,
                    'error': ollama_result['error'],
                    'extracted_text': extracted_text[:500] + "..." if len(extracted_text) > 500 else extracted_text,
                    'image_filename': image_filename,
                    'ollama_details': ollama_result
                }
                
        except Exception as e:
            logger.error(f"Processing error for {image_filename}: {e}")
            return {
                'success': False,
                'error': f'Processing error: {str(e)}',
                'image_filename': image_filename,
                'traceback': traceback.format_exc()
            }
    
    def process_pdf(self, pdf_path):
        """Process PDF with comprehensive error handling"""
        pdf_filename = os.path.basename(pdf_path)
        
        try:
            logger.info(f"Processing PDF: {pdf_filename}")
            
            # Validate PDF first
            is_valid, validation_msg = self._validate_pdf(pdf_path)
            if not is_valid:
                return {
                    'success': False,
                    'error': f'PDF validation failed: {validation_msg}',
                    'pdf_filename': pdf_filename
                }
            
            # Extract text from PDF
            extracted_text, extraction_details = self.extract_text_from_pdf(pdf_path)
            
            if not extracted_text.strip():
                return {
                    'success': False,
                    'error': 'No readable text found in PDF',
                    'pdf_filename': pdf_filename,
                    'extraction_details': extraction_details
                }
            
            # Generate structured JSON
            ollama_result = self.generate_json_with_ollama(extracted_text, pdf_filename)
            
            if ollama_result['success']:
                return {
                    'success': True,
                    'pdf_filename': pdf_filename,
                    'extracted_text': extracted_text,
                    'extraction_details': extraction_details,
                    'structured_json': ollama_result['json_data'],
                    'text_blocks_count': len(extraction_details),
                    'text_confidence_avg': sum(item['confidence'] for item in extraction_details) / len(extraction_details) if extraction_details else 100,
                    'file_type': 'pdf'
                }
            else:
                return {
                    'success': False,
                    'error': ollama_result['error'],
                    'extracted_text': extracted_text[:500] + "..." if len(extracted_text) > 500 else extracted_text,
                    'pdf_filename': pdf_filename,
                    'ollama_details': ollama_result
                }
                
        except Exception as e:
            logger.error(f"PDF processing error for {pdf_filename}: {e}")
            return {
                'success': False,
                'error': f'PDF processing error: {str(e)}',
                'pdf_filename': pdf_filename,
                'traceback': traceback.format_exc()
            }

# ================================
# INITIALIZATION FUNCTIONS
# ================================

@st.cache_resource
def init_qdrant():
    """Initialize Qdrant Cloud client with better error handling"""
    try:
        qdrant_url = st.secrets.get("QDRANT_URL") or os.getenv("QDRANT_URL")
        qdrant_api_key = st.secrets.get("QDRANT_API_KEY") or os.getenv("QDRANT_API_KEY")
        
        if not qdrant_url or not qdrant_api_key:
            st.error("‚ùå Qdrant Cloud credentials not found in secrets or environment!")
            st.info("Please add QDRANT_URL and QDRANT_API_KEY to your Streamlit secrets or environment variables")
            st.stop()
        
        client = qdrant_client.QdrantClient(url=qdrant_url, api_key=qdrant_api_key)
        
        # Test connection
        collections = client.get_collections()
        st.success(f"‚úÖ Connected to Qdrant Cloud - {len(collections.collections)} collections found")
        return client
        
    except Exception as e:
        st.error(f"‚ùå Cannot connect to Qdrant Cloud: {str(e)}")
        st.info("Please check your Qdrant Cloud credentials and connection")
        st.stop()

@st.cache_resource
def init_embedding():
    """Initialize embedding model with error handling"""
    try:
        return HuggingFaceEmbedding(model_name="BAAI/bge-large-en-v1.5", trust_remote_code=True)
    except Exception as e:
        st.error(f"Failed to initialize embedding model: {e}")
        st.stop()

@st.cache_resource
def init_mistral():
    """Initialize Mistral AI model with error handling"""
    try:
        api_key = st.secrets.get("MISTRAL_API_KEY") or os.getenv("MISTRAL_API_KEY")
        if not api_key:
            st.error("‚ùå MISTRAL_API_KEY not found!")
            st.info("Please add MISTRAL_API_KEY to your Streamlit secrets or environment variables")
            st.stop()
        return MistralAI(model="mistral-large-latest", temperature=0.7, max_tokens=512, api_key=api_key)
    except Exception as e:
        st.error(f"Failed to initialize Mistral AI: {e}")
        st.stop()

@st.cache_resource
def init_ocr_processor():
    """Initialize OCR processor with error handling"""
    try:
        return MedicalReportOCR()
    except Exception as e:
        st.error(f"Failed to initialize OCR processor: {e}")
        raise

# ================================
# HELPER FUNCTIONS
# ================================

def save_uploaded_file(uploaded_file):
    """Save uploaded file to temporary location"""
    try:
        # Create temporary file with proper extension
        file_ext = uploaded_file.name.split('.')[-1].lower()
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_ext}") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            return tmp_file.name
    except Exception as e:
        logger.error(f"Error saving uploaded file: {e}")
        raise

def display_processing_results(processed_reports):
    """Display detailed processing results"""
    successful = [r for r in processed_reports if r.get('success', False)]
    failed = [r for r in processed_reports if not r.get('success', False)]
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("üìÑ Total Files", len(processed_reports))
    with col2:
        st.metric("‚úÖ Successfully Processed", len(successful))
    with col3:
        st.metric("‚ùå Failed", len(failed))
    
    # Show failed reports with details
    if failed:
        st.warning("‚ö†Ô∏è Some files failed to process:")
        for report in failed:
            with st.expander(f"‚ùå {report.get('image_filename', 'Unknown file')} - Error Details"):
                st.error(f"**Error:** {report.get('error', 'Unknown error')}")
                
                if 'extracted_text' in report and report['extracted_text']:
                    st.write("**Extracted Text (partial):**")
                    st.text(report['extracted_text'][:300] + "..." if len(report['extracted_text']) > 300 else report['extracted_text'])
                
                if 'ollama_details' in report:
                    st.write("**Ollama Processing Details:**")
                    st.json(report['ollama_details'])
    
    return len(successful) > 0

# ================================
# DATABASE FUNCTIONS (Simplified)
# ================================

def create_documents_from_json_data(json_reports):
    """Create documents from JSON data with error handling"""
    documents = []
    
    for report in json_reports:
        if not report.get('success', False):
            logger.warning(f"Skipping failed report: {report.get('original_filename', 'unknown')}")
            continue
        
        try:
            json_data = report.get('structured_json', {})
            if not json_data:
                logger.warning(f"No structured JSON data for report: {report.get('original_filename', 'unknown')}")
                continue
            
            # Create text content
            text_parts = []
            
            # Hospital info
            hospital_info = json_data.get('hospital_info', {})
            if hospital_info and hospital_info.get('hospital_name'):
                text_parts.append(f"Hospital: {hospital_info['hospital_name']}")
            
            # Patient info
            patient_info = json_data.get('patient_info', {})
            if patient_info:
                if patient_info.get('name'):
                    text_parts.append(f"Patient: {patient_info['name']}")
                if patient_info.get('age'):
                    text_parts.append(f"Age: {patient_info['age']}")
                if patient_info.get('gender'):
                    text_parts.append(f"Gender: {patient_info['gender']}")
            
            # Report info
            report_info = json_data.get('report_info', {})
            if report_info and report_info.get('report_type'):
                text_parts.append(f"Report Type: {report_info['report_type']}")
            
            # Test results
            test_results = json_data.get('test_results', [])
            if isinstance(test_results, list) and test_results:
                for test in test_results:
                    if isinstance(test, dict) and test.get('test_name'):
                        test_text = f"Test: {test['test_name']}"
                        if test.get('result_value'):
                            test_text += f" Result: {test['result_value']}"
                        if test.get('unit'):
                            test_text += f" {test['unit']}"
                        text_parts.append(test_text)
            
            # Add original extracted text if available
            if 'extracted_text' in report and report['extracted_text']:
                text_parts.append(f"Original Text: {report['extracted_text']}")
            
            # Ensure we have some content
            if not text_parts:
                text_parts.append(f"Medical report from {report.get('original_filename', 'unknown source')}")
            
            text_content = "\n".join(text_parts)
            
            # Create document with safe metadata extraction
            metadata = {
                'source_image': report.get('original_filename', report.get('image_filename', 'unknown')),
                'patient_name': patient_info.get('name', 'Unknown') if patient_info else 'Unknown',
                'hospital_name': hospital_info.get('hospital_name', 'Unknown') if hospital_info else 'Unknown',
                'report_type': report_info.get('report_type', 'Medical Report') if report_info else 'Medical Report',
                'processing_timestamp': json_data.get('_metadata', {}).get('processing_timestamp', ''),
                'test_count': len(test_results) if isinstance(test_results, list) else 0,
                'file_hash': report.get('file_hash', '')  # Add file hash for duplicate detection
            }
            
            document = Document(
                text=text_content,
                metadata=metadata
            )
            documents.append(document)
            logger.info(f"Created document for: {report.get('original_filename', 'unknown')}")
            
        except Exception as e:
            logger.error(f"Error creating document from report {report.get('original_filename', 'unknown')}: {e}")
            logger.error(f"Report data: {str(report)[:200]}...")
            continue
    
    logger.info(f"Created {len(documents)} documents from {len(json_reports)} reports")
    return documents

def get_file_hash(file_content):
    """Generate hash for file content to detect duplicates"""
    return hashlib.md5(file_content).hexdigest()

def check_existing_documents(client, collection_name, file_hashes):
    """Check which documents already exist in the database"""
    try:
        # Try to get collection info
        collection_info = client.get_collection(collection_name)
        
        # Scroll through existing points to check for duplicates
        existing_hashes = set()
        points, _ = client.scroll(collection_name, limit=1000)
        
        for point in points:
            if point.payload and 'file_hash' in point.payload:
                existing_hashes.add(point.payload['file_hash'])
        
        # Return which files are new
        new_file_hashes = [h for h in file_hashes if h not in existing_hashes]
        return new_file_hashes, existing_hashes
        
    except Exception as e:
        logger.info(f"Collection doesn't exist or error checking: {e}")
        return file_hashes, set()  # Assume all files are new


def setup_database_from_json(json_reports, client, collection_name):
    """Setup database with incremental updates - no data loss"""
    try:
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # Filter only successful reports
        successful_reports = [r for r in json_reports if r.get('success', False)]
        if not successful_reports:
            return False, "No successful reports to process"
        
        # Generate file hashes to check for duplicates
        file_hashes = []
        for report in successful_reports:
            file_content = report.get('extracted_text', '') + report.get('original_filename', '')
            file_hash = get_file_hash(file_content.encode('utf-8'))
            report['file_hash'] = file_hash
            file_hashes.append(file_hash)
        
        status_text.text("üîç Checking for existing documents...")
        progress_bar.progress(10)
        
        # Check which files are already in database
        new_file_hashes, existing_hashes = check_existing_documents(client, collection_name, file_hashes)
        
        # Filter to only new reports
        new_reports = [r for r in successful_reports if r['file_hash'] in new_file_hashes]
        
        if not new_reports:
            status_text.text("‚ÑπÔ∏è All files already exist in database")
            progress_bar.progress(100)
            return True, f"All {len(successful_reports)} files already exist in database. Skipped duplicates."
        
        status_text.text(f"üìÑ Processing {len(new_reports)} new reports (skipping {len(existing_hashes)} duplicates)...")
        progress_bar.progress(20)
        
        documents = create_documents_from_json_data(new_reports)
        
        if not documents:
            return False, "No valid documents created from new reports"
        
        status_text.text("üîÑ Initializing embedding model...")
        progress_bar.progress(40)
        embed_model = init_embedding()
        
        status_text.text("üîÑ Setting up vector store...")
        progress_bar.progress(60)
        
        # DON'T delete existing collection - just connect to it
        vector_store = QdrantVectorStore(client=client, collection_name=collection_name)
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        
        status_text.text("üîÑ Adding new documents to existing database...")
        progress_bar.progress(80)
        
        # Add documents to existing index
        try:
            # Try to load existing index
            existing_index = VectorStoreIndex.from_vector_store(
                vector_store, 
                storage_context=storage_context, 
                embed_model=embed_model
            )
            
            # Insert new documents
            for doc in documents:
                existing_index.insert(doc)
                
        except Exception as e:
            # If no existing index, create new one
            logger.info(f"Creating new index: {e}")
            index = VectorStoreIndex.from_documents(
                documents, 
                storage_context=storage_context, 
                embed_model=embed_model, 
                show_progress=False
            )
        
        progress_bar.progress(100)
        status_text.text("‚úÖ Database updated successfully!")
        
        total_in_db = len(existing_hashes) + len(new_reports)
        return True, f"Successfully added {len(new_reports)} new reports! Total in database: {total_in_db}"
        
    except Exception as e:
        logger.error(f"Database setup error: {e}")
        return False, f"Error updating database: {str(e)}"

def init_query_engine(_client, collection_name):
    """Initialize the query engine for RAG"""

    try:
        try:
            collection_info = _client.get_collection(collection_name)
            if collection_info.points_count == 0:
                raise Exception("No data in collection")
        except Exception as e:
            raise Exception(f"Collection not ready: {str(e)}")
        embed_model = init_embedding()
        llm = init_mistral()
        Settings.embed_model = embed_model
        Settings.llm = llm
        
        vector_store = QdrantVectorStore(client=_client, collection_name=collection_name)
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context, embed_model=embed_model)
        
        rerank = SentenceTransformerRerank(model="cross-encoder/ms-marco-MiniLM-L-2-v2", top_n=5)
        
        template = """Context information from medical reports:
                    ---------------------
                    {context_str}
                    ---------------------
                    
Based on the medical reports data above, provide accurate answers to questions about:
- Patient demographics and counts
- Test results and abnormal values
- Hospital information and report types
- Date ranges and temporal queries
- Statistical summaries and trends

If you cannot find specific information in the reports, clearly state that the information is not available.

Question: {query_str}

Answer:"""
        
        qa_prompt_tmpl = PromptTemplate(template)
        query_engine = index.as_query_engine(llm=llm, similarity_top_k=10, node_postprocessors=[rerank])
        query_engine.update_prompts({"response_synthesizer:text_qa_template": qa_prompt_tmpl})
        
        return query_engine
        
    except Exception as e:
        st.error(f"Error initializing query engine: {str(e)}")
        raise e

# ================================
# MAIN STREAMLIT APP
# ================================
@st.cache_resource
def get_cached_query_engine(_client, collection_name, _force_refresh=False):
    """Cached query engine that only refreshes when needed"""
    return init_query_engine(_client, collection_name)

def main():
    # Header
    st.markdown("""
    <div class="main-header">
        <h1>üè• Medical Report Analytics System</h1>
        <p style="font-size: 1.2em; margin-top: 10px;">AI-Powered Medical Report Processing & Analysis</p>
        <p style="opacity: 0.9;">Upload medical reports ‚Üí Extract data ‚Üí Ask intelligent questions!</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Initialize components
    client = init_qdrant()
    collection_name = "medical_reports_db"
    
    # ================================
    # SIDEBAR: File Upload & Processing
    # ================================
    
    with st.sidebar:
        st.markdown("### üì§ Upload Medical Reports")
        
        uploaded_files = st.file_uploader(
            "Choose medical report files",
            type=['png', 'jpg', 'jpeg', 'pdf'],  # Added PDF support
            accept_multiple_files=True,
            help="Upload medical report images (PNG, JPG, JPEG) or PDF documents"
        )
        
        if uploaded_files:
            st.success(f"üìÅ {len(uploaded_files)} files uploaded")
            image_count = sum(1 for f in uploaded_files if f.name.lower().endswith(('.png', '.jpg', '.jpeg')))
            pdf_count = sum(1 for f in uploaded_files if f.name.lower().endswith('.pdf'))
            
            if image_count > 0:
                st.info(f"üñºÔ∏è Images: {image_count}")
            if pdf_count > 0:
                st.info(f"üìÑ PDFs: {pdf_count}")
            # Ollama status check
            ollama_status = st.empty()
            try:
                response = requests.get("http://localhost:11434/api/tags", timeout=5)
                if response.status_code == 200:
                    ollama_status.success("ü§ñ Ollama: Connected")
                else:
                    ollama_status.error("‚ùå Ollama: Not connected")
            except:
                ollama_status.error("‚ùå Ollama: Not running")
            
            if st.button("üöÄ Process All Reports", use_container_width=True):
                if len(uploaded_files) == 0:
                    st.error("Please upload at least one file")
                    return
        
    # Initialize OCR processor
    try:
        ocr_processor = init_ocr_processor()
    except Exception as e:
        st.error(f"Failed to initialize OCR: {e}")
        return
    
    # Process all uploaded files
    with st.spinner("Processing medical reports..."):
        processed_reports = []
        progress_bar = st.progress(0)
        
        for i, uploaded_file in enumerate(uploaded_files):
            st.text(f"Processing: {uploaded_file.name}")
            
            try:
                # Save uploaded file temporarily with correct extension
                file_ext = uploaded_file.name.split('.')[-1].lower()
                with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_ext}") as tmp_file:
                    tmp_file.write(uploaded_file.getvalue())
                    tmp_path = tmp_file.name
                
                # Process the file (automatically detects if it's PDF or image)
                result = ocr_processor.process_file(tmp_path)
                result['original_filename'] = uploaded_file.name
                processed_reports.append(result)
                
                # Clean up temp file
                try:
                    os.unlink(tmp_path)
                except:
                    pass
                    
            except Exception as e:
                # Add error report for failed files
                error_result = {
                    'success': False,
                    'error': f'File processing error: {str(e)}',
                    'original_filename': uploaded_file.name,
                    'image_filename': uploaded_file.name  # For compatibility
                }
                processed_reports.append(error_result)
                logger.error(f"Error processing {uploaded_file.name}: {e}")
            
            # Update progress
            progress_bar.progress((i + 1) / len(uploaded_files))
        
        # Store processed reports in session state
        st.session_state.processed_reports = processed_reports
        
        # Show processing results with details
        successful = sum(1 for r in processed_reports if r.get('success', False))
        failed = len(processed_reports) - successful
        
        if successful > 0:
            st.success(f"‚úÖ Successfully processed {successful} reports")
            
            # Setup database
            try:
                success, message = setup_database_from_json(processed_reports, client, collection_name)
                if success:
                    st.success("üîÑ Database updated!")
                    get_cached_query_engine.clear()
                    st.cache_resource.clear()
                else:
                    st.error(f"Database error: {message}")
            except Exception as e:
                st.error(f"Database setup failed: {str(e)}")
        
        if failed > 0:
            st.warning(f"‚ö†Ô∏è Failed to process {failed} reports")
            
            # Show detailed error information
            with st.expander("View Error Details"):
                for report in processed_reports:
                    if not report.get('success', False):
                        st.error(f"**{report.get('original_filename', 'Unknown')}**: {report.get('error', 'Unknown error')}")

        
        # Database status
        st.markdown("---")
        st.markdown("### üìä Database Status")
        
        collections = client.get_collections()
        collection_names = [col.name for col in collections.collections]
        
        if collection_name in collection_names:
            st.success("‚úÖ Database Ready")
            try:
                collection_info = client.get_collection(collection_name)
                st.metric("üìÑ Reports", collection_info.points_count)
            except:
                pass
        else:
            st.warning("‚ö†Ô∏è No data yet")
            st.info("üëÜ Upload reports to get started")
    
    # ================================
    # MAIN AREA: Chat Interface
    # ================================
    collection_has_data = False
    if collection_name in collection_names:
        try:
            collection_info = client.get_collection(collection_name)
            collection_has_data = collection_info.points_count > 0
        except:
            collection_has_data = False
        
    if not collection_has_data:
    # Check if database exists
        if collection_name not in collection_names:
            st.markdown("""
            <div style="text-align: center; padding: 3rem;">
                <h3>üöÄ Welcome to Medical Report Analytics</h3>
                <p>Upload medical report images using the sidebar to begin analysis!</p>
                <p>Once processed, you can ask questions like:</p>
                <ul style="text-align: left; max-width: 600px; margin: 0 auto;">
                    <li>How many patients' data is available?</li>
                    <li>What are the abnormal test results?</li>
                    <li>Show me reports from a specific hospital</li>
                    <li>Which patients have high blood sugar levels?</li>
                </ul>
            </div>
            """, unsafe_allow_html=True)
            return
    
    # Initialize query engine
    try:
        # Use cached query engine
        query_engine = get_cached_query_engine(client, collection_name)
    except Exception as e:
        st.error(f"‚ùå Failed to initialize AI: {str(e)}")
        return
    
    # Chat interface
    tab1, tab2, tab3 = st.tabs(["üí¨ Ask Questions", "üìä Report Summary", "üîç Sample Queries"])
    
    with tab1:
        # Initialize chat history
        if "medical_messages" not in st.session_state:
            st.session_state.medical_messages = []
            st.session_state.medical_messages.append({
                "role": "assistant",
                "content": "üëã Hello! I'm your Medical Report Analytics Assistant. I can help you analyze the processed medical reports. Ask me anything about the patient data, test results, or hospital information!"
            })
        
        # Display chat history
        for message in st.session_state.medical_messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
        
        # Chat input
        if prompt := st.chat_input("üí¨ Ask about the medical reports data..."):
            st.session_state.medical_messages.append({"role": "user", "content": prompt})
            
            with st.chat_message("user"):
                st.markdown(prompt)
            
            with st.chat_message("assistant"):
                with st.spinner("üîç Analyzing medical data..."):
                    try:
                        response = query_engine.query(prompt)
                        st.markdown(str(response))
                        st.session_state.medical_messages.append({"role": "assistant", "content": str(response)})
                    except Exception as e:
                        error_msg = f"‚ùå Sorry, I encountered an error: {str(e)}"
                        st.error(error_msg)
                        st.session_state.medical_messages.append({"role": "assistant", "content": error_msg})
    
    with tab2:
        st.markdown("### üìä Processing Summary")
        
        if hasattr(st.session_state, 'processed_reports'):
            reports = st.session_state.processed_reports
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                total_reports = len(reports)
                st.metric("üìÑ Total Reports", total_reports)
            
            with col2:
                successful = sum(1 for r in reports if r.get('success', False))
                st.metric("‚úÖ Successfully Processed", successful)
            
            with col3:
                failed = total_reports - successful
                st.metric("‚ùå Failed", failed)
            
            # Show detailed results
            if successful > 0:
                st.markdown("### üìã Processed Reports Details")
                
                for i, report in enumerate(reports):
                    if report.get('success', False):
                        with st.expander(f"üìë {report.get('original_filename', f'Report {i+1}')}"):
                            json_data = report['structured_json']
                            
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                st.markdown("**Patient Info:**")
                                patient_info = json_data.get('patient_info', {})
                                st.write(f"‚Ä¢ Name: {patient_info.get('name', 'N/A')}")
                                st.write(f"‚Ä¢ Age: {patient_info.get('age', 'N/A')}")
                                st.write(f"‚Ä¢ Gender: {patient_info.get('gender', 'N/A')}")
                            
                            with col2:
                                st.markdown("**Hospital Info:**")
                                hospital_info = json_data.get('hospital_info', {})
                                st.write(f"‚Ä¢ Hospital: {hospital_info.get('hospital_name', 'N/A')}")
                                st.write(f"‚Ä¢ Report Type: {json_data.get('report_info', {}).get('report_type', 'N/A')}")
                            
                            # Test results count
                            test_results = json_data.get('test_results', [])
                            if isinstance(test_results, list):
                                st.write(f"**Tests Conducted:** {len(test_results)}")
        else:
            st.info("No processed reports yet. Upload and process some medical reports first!")
    
    with tab3:
        st.markdown("### üîç Try These Sample Queries")
        
        sample_queries = [
            "üìä How many patients' data is available in the system?",
            "üè• Which hospitals are represented in the reports?",
            "üß™ What types of medical tests were conducted?",
            "üìÖ Show me reports from the last month",
            "‚ö†Ô∏è Are there any abnormal test results?",
            "üë• What's the age distribution of patients?",
            "üî¨ List all blood test results",
            "üìà Show me patients with high glucose levels"
        ]
        
        for i, query in enumerate(sample_queries):
            if st.button(query, key=f"query_{i}", use_container_width=True):
                # Initialize messages if not exists
                if "medical_messages" not in st.session_state:
                    st.session_state.medical_messages = []
                
                # Add query to chat
                st.session_state.medical_messages.append({"role": "user", "content": query})
                
                # Generate response
                try:
                    with st.spinner("üîç Processing query..."):
                        response = query_engine.query(query)
                        st.session_state.medical_messages.append({"role": "assistant", "content": str(response)})
                    st.success("‚úÖ Query processed! Check the 'Ask Questions' tab.")
                except Exception as e:
                    error_msg = f"‚ùå Sorry, I encountered an error: {str(e)}"
                    st.session_state.medical_messages.append({"role": "assistant", "content": error_msg})
                    st.error("Failed to process query.")
                
                # Refresh to show conversation
                st.rerun()

if __name__ == "__main__":
    main()
